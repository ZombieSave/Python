{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f01a3b",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "Импортируйте библиотеки pandas и numpy.\n",
    "Загрузите \"Boston House Prices dataset\" из встроенных наборов данных библиотеки sklearn. Создайте датафреймы X и y из этих данных.\n",
    "Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки\n",
    "составлял 30% от всех данных, при этом аргумент random_state должен быть равен 42.\n",
    "Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model.\n",
    "Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых.\n",
    "Вычислите R2 полученных предказаний с помощью r2_score из модуля sklearn.metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbef39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0025f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "columns = boston[\"feature_names\"]\n",
    "X = pd.DataFrame(boston[\"data\"], columns=boston[\"feature_names\"])\n",
    "y = pd.DataFrame(boston[\"target\"], columns=[\"Price\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d1ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_R2 = 0.711226005748496\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(f\"lr_R2 = {r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee69116",
   "metadata": {},
   "source": [
    "#### Задание 2\n",
    "Создайте модель под названием model с помощью RandomForestRegressor из модуля sklearn.ensemble.\n",
    "Сделайте агрумент n_estimators равным 1000,\n",
    "max_depth должен быть равен 12 и random_state сделайте равным 42.\n",
    "Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression,\n",
    "но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0],\n",
    "чтобы получить из датафрейма одномерный массив Numpy,\n",
    "так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно применение массивов вместо датафрейма.\n",
    "Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из предыдущего задания.\n",
    "Напишите в комментариях к коду, какая модель в данном случае работает лучше.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b501abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db698d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfr_R2 = 0.87472606157312\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)\n",
    "rfr.fit(X_train, y_train.values[:, 0])\n",
    "y_pred = rfr.predict(X_test)\n",
    "print(f\"rfr_R2 = {r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0829bd",
   "metadata": {},
   "source": [
    "Ответ: поскольку чем ближе коэффициент детерминации R2 к 1 тем лучше измеренные данные соотвествуют модели то модель RandomForestRegressor работает лучше чем линейная регрессия (для данного набора данных): \n",
    "rfr_R2  > lr_R2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb09e5",
   "metadata": {},
   "source": [
    "#### *Задание 3\n",
    "Вызовите документацию для класса RandomForestRegressor,\n",
    "найдите информацию об атрибуте feature_importances_.\n",
    "С помощью этого атрибута найдите сумму всех показателей важности,\n",
    "установите, какие два признака показывают наибольшую важность.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3aeeb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сумма всех показателей равна 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "feats = {}\n",
    "for feature_name, importance in zip(boston[\"feature_names\"], rfr.feature_importances_):\n",
    "    feats[feature_name] = importance\n",
    "    \n",
    "feature_importsnce_df = pd.DataFrame.from_dict(feats, orient='index')\n",
    "print(f\"Сумма всех показателей равна {rfr.feature_importances_.sum()}\")\n",
    "feature_importsnce_df.plot(kind=\"bar\")\n",
    "plt.title('Feature importances', fontsize=20, weight = 'bold')\n",
    "plt.xlabel('Feature', fontsize=15)\n",
    "plt.ylabel('Importance', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252ac1e",
   "metadata": {},
   "source": [
    "Ответ: признаки RM и LSTAT имеют наибольшую важность."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97011284",
   "metadata": {},
   "source": [
    "\n",
    "#### *Задание 4\n",
    "В этом задании мы будем работать с датасетом, с которым мы уже знакомы по домашнему заданию по библиотеке Matplotlib, это датасет Credit Card Fraud Detection.Для этого датасета мы будем решать задачу классификации - будем определять,какие из транзакциции по кредитной карте являются мошенническими.Данный датасет сильно несбалансирован (так как случаи мошенничества относительно редки),так что применение метрики accuracy не принесет пользы и не поможет выбрать лучшую модель.Мы будем вычислять AUC, то есть площадь под кривой ROC.\n",
    "Импортируйте из соответствующих модулей RandomForestClassifier, GridSearchCV и train_test_split.\n",
    "Загрузите датасет creditcard.csv и создайте датафрейм df.\n",
    "С помощью метода value_counts с аргументом normalize=True убедитесь в том, что выборка несбалансирована.Используя метод info, проверьте, все ли столбцы содержат числовые данные и нет ли в них пропусков.Примените следующую настройку, чтобы можно было просматривать все столбцы датафрейма:\n",
    "pd.options.display.max_columns = 100.\n",
    "Просмотрите первые 10 строк датафрейма df.\n",
    "Создайте датафрейм X из датафрейма df, исключив столбец Class.\n",
    "Создайте объект Series под названием y из столбца Class.\n",
    "Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split, используя аргументы: test_size=0.3, random_state=100, stratify=y.\n",
    "У вас должны получиться объекты X_train, X_test, y_train и y_test.\n",
    "Просмотрите информацию о их форме.\n",
    "Для поиска по сетке параметров задайте такие параметры:\n",
    "parameters = [{'n_estimators': [10, 15],\n",
    "'max_features': np.arange(3, 5),\n",
    "'max_depth': np.arange(4, 7)}]\n",
    "Создайте модель GridSearchCV со следующими аргументами:\n",
    "estimator=RandomForestClassifier(random_state=100),\n",
    "param_grid=parameters,\n",
    "scoring='roc_auc',\n",
    "cv=3.\n",
    "Обучите модель на тренировочном наборе данных (может занять несколько минут).\n",
    "Просмотрите параметры лучшей модели с помощью атрибута best_params_.\n",
    "Предскажите вероятности классов с помощью полученнной модели и метода predict_proba.\n",
    "Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и запишите в массив y_pred_proba. Из модуля sklearn.metrics импортируйте метрику roc_auc_score.\n",
    "Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных, используя в качестве аргументов массивы y_test и y_pred_proba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0e84f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>1.341262</td>\n",
       "      <td>0.359894</td>\n",
       "      <td>-0.358091</td>\n",
       "      <td>-0.137134</td>\n",
       "      <td>0.517617</td>\n",
       "      <td>0.401726</td>\n",
       "      <td>-0.058133</td>\n",
       "      <td>0.068653</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>-1.416907</td>\n",
       "      <td>-0.153826</td>\n",
       "      <td>-0.751063</td>\n",
       "      <td>0.167372</td>\n",
       "      <td>0.050144</td>\n",
       "      <td>-0.443587</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.611987</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>-0.619468</td>\n",
       "      <td>0.291474</td>\n",
       "      <td>1.757964</td>\n",
       "      <td>-1.323865</td>\n",
       "      <td>0.686133</td>\n",
       "      <td>-0.076127</td>\n",
       "      <td>-1.222127</td>\n",
       "      <td>-0.358222</td>\n",
       "      <td>0.324505</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>-0.705117</td>\n",
       "      <td>-0.110452</td>\n",
       "      <td>-0.286254</td>\n",
       "      <td>0.074355</td>\n",
       "      <td>-0.328783</td>\n",
       "      <td>-0.210077</td>\n",
       "      <td>-0.499768</td>\n",
       "      <td>0.118765</td>\n",
       "      <td>0.570328</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>1.017614</td>\n",
       "      <td>0.836390</td>\n",
       "      <td>1.006844</td>\n",
       "      <td>-0.443523</td>\n",
       "      <td>0.150219</td>\n",
       "      <td>0.739453</td>\n",
       "      <td>-0.540980</td>\n",
       "      <td>0.476677</td>\n",
       "      <td>0.451773</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
       "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
       "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
       "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
       "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
       "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
       "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
       "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
       "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
       "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
       "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
       "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
       "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  \n",
       "5    3.67      0  \n",
       "6    4.99      0  \n",
       "7   40.80      0  \n",
       "8   93.20      0  \n",
       "9    3.68      0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "df = pd.read_csv(\"c:\\\\creditcard.csv\")\n",
    "print(df[\"Class\"].value_counts())\n",
    "df.info()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2571de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (199364, 30)\n",
      "X_test.shape (85443, 30)\n",
      "y_train.shape (199364,)\n",
      "y_test.shape (85443,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=100),\n",
       "             param_grid={'max_depth': array([4, 5, 6]),\n",
       "                         'max_features': array([3, 4]),\n",
       "                         'n_estimators': [10, 15]},\n",
       "             return_train_score=True, scoring='roc_auc')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop([\"Class\"], axis=1)\n",
    "y = df[\"Class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)\n",
    "\n",
    "print(f\"X_train.shape {X_train.shape}\")\n",
    "print(f\"X_test.shape {X_test.shape}\")\n",
    "print(f\"y_train.shape {y_train.shape}\")\n",
    "print(f\"y_test.shape {y_test.shape}\")\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': [10, 15],\n",
    "    'max_features': np.arange(3, 5),\n",
    "    'max_depth': np.arange(4, 7),\n",
    "}\n",
    "gscv = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=100),\n",
    "    param_grid=parameters,\n",
    "    scoring='roc_auc',\n",
    "    return_train_score=True,\n",
    "    cv=3\n",
    ")\n",
    "gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e0afe9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты RandomForestClassifier с лучшими параметрами (строка с индексом 9 из gscv_results): {'max_depth': 6, 'max_features': 3, 'n_estimators': 15}\n",
      "mean_fit_time                                                    3.3529\n",
      "std_fit_time                                                  0.0855341\n",
      "mean_score_time                                               0.0829991\n",
      "std_score_time                                              0.000810014\n",
      "param_max_depth                                                       6\n",
      "param_max_features                                                    3\n",
      "param_n_estimators                                                   15\n",
      "params                {'max_depth': 6, 'max_features': 3, 'n_estimat...\n",
      "split0_test_score                                              0.974552\n",
      "split1_test_score                                              0.959064\n",
      "split2_test_score                                               0.96429\n",
      "mean_test_score                                                0.965969\n",
      "std_test_score                                               0.00643356\n",
      "rank_test_score                                                       1\n",
      "split0_train_score                                             0.972536\n",
      "split1_train_score                                             0.976009\n",
      "split2_train_score                                             0.980076\n",
      "mean_train_score                                               0.976207\n",
      "std_train_score                                               0.0030815\n",
      "Name: 9, dtype: object\n",
      "\n",
      "roc_auc_score 0.9462664156037156\n"
     ]
    }
   ],
   "source": [
    "gscv_results = pd.DataFrame(gscv.cv_results_)\n",
    "print(f\"Результаты RandomForestClassifier с лучшими параметрами (строка с индексом 9 из gscv_results): {gscv.best_params_}\")\n",
    "# print(gscv_results)\n",
    "print(gscv_results.loc[9])\n",
    "print(\"\")\n",
    "proba = gscv.predict_proba(X_test)\n",
    "y_pred_proba = proba[:, 1]\n",
    "print(f\"roc_auc_score {roc_auc_score(y_test, y_pred_proba)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60116e93",
   "metadata": {},
   "source": [
    "Ответ: значение AUC, вычисленное при помощи roc_auc_score на тестовых данных: 0.9462664156037156, немного хуже чем вычисленное на тренировочных данных при помощи RandomForestClassifier и best parameters (max_depth': 6, 'max_features': 3, 'n_estimators': 15): mean_train_score=0.976207. Поскольку разница между значениями несущественная, переобучения модели не наблюдается."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
